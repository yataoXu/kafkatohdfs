server:
  port: 8088 #设置一个默认组

spring:
  messages:
    basename: i18n/Messages,i18n/Pages
  kafka:
    bootstrap-servers: 10.2.196.18:9092,10.2.196.19:9092,10.2.196.20:9092
    template:
      default-topic: self-topic0
    consumer:
      key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      value-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      group-id: myGroup998
      # 最早未被消费的offset
      auto-offset-reset: earliest
      # 批量一次最大拉取数据量
      max-poll-records: 1000
      # 自动提交
      enable-auto-commit: false
    consumer-extra:
      # 是否批量处理
      batch-listener: true

hive:
  url: jdbc:hive2://10.10.1.141:10000/hive
  driver-class-name: org.apache.hive.jdbc.HiveDriver
  type: com.alibaba.druid.pool.DruidDataSource
  user: bi
  password: rojao123
  # 下面为连接池的补充设置，应用到上面所有数据源中
  # 初始化大小，最小，最大
  initialSize: 1
  minIdle: 3
  maxActive: 20
  # 配置获取连接等待超时的时间
  maxWait: 60000
  # 配置间隔多久才进行一次检测，检测需要关闭的空闲连接，单位是毫秒
  timeBetweenEvictionRunsMillis: 60000
  # 配置一个连接在池中最小生存的时间，单位是毫秒
  minEvictableIdleTimeMillis: 30000
  validationQuery: select 1
  testWhileIdle: true
  testOnBorrow: false
  testOnReturn: false
  # 打开PSCache，并且指定每个连接上PSCache的大小
  poolPreparedStatements: true
  maxPoolPreparedStatementPerConnectionSize: 20

hadoop.name-node: hdfs://10.2.196.18:9000
hadoop.namespace: /